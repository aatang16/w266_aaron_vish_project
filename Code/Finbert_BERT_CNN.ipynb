{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ystJX_zbHMSf"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "\n",
        "!pip install pydot --quiet\n",
        "!pip install gensim --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install -U tensorflow-text --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UC22FJaAHNt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70ddbc3-4ce3-4785-8a10-182f553eb9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#!pip install tensorflow_text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "#import tensorflow_text as tf_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import ne_chunk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "from collections import Counter\n",
        "stopwords = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EsK3jjqgLQLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OJ5fZmoFJpzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fbaea7-4590-4702-cbb2-7da553b50c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Label                                      Risk Sentence\n",
            "0          0  risk factor risk factor normal course business...\n",
            "1          0  following discussion material risk factor appl...\n",
            "2          0  additional information risk management framewo...\n",
            "3          0  additional risk beyond described management di...\n",
            "4          0  strategic risk consummation planned acquisitio...\n",
            "...      ...                                                ...\n",
            "25532      2  dilution reduction delay accretion schwabs ear...\n",
            "25533      2  pending merger td ameritrade completed stockho...\n",
            "25534      2  proposed merger completed issue td ameritrade ...\n",
            "25535      2  result issuance share common stock stockholder...\n",
            "25536      2        addition td bank become largest stockholder\n",
            "\n",
            "[25537 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_paths = ['/content/AM.csv', '/content/CF.csv', '/content/DB.csv', '/content/IB.csv']\n",
        "\n",
        "df = [pd.read_csv(file_path) for file_path in file_paths]\n",
        "\n",
        "merged = pd.concat(df, ignore_index = True)\n",
        "\n",
        "print(merged)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged['Label'] = merged['Label'].astype(int)\n",
        "label_counts = merged['Label'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tek8N7doUjne",
        "outputId": "c4643d76-cb97-48bf-98a6-074bd60bed91"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    7020\n",
            "3    6610\n",
            "0    6325\n",
            "1    5582\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Stop words\n",
        "from nltk.corpus import stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords_list=stopwords.words('english')\n",
        "stopwords_list.extend(['this','that','where','what','when','why','which','the','they','him','her','includes_bibliographical_references_index'])\n",
        "\n",
        "#add additional stopwords to be removed"
      ],
      "metadata": {
        "id": "J47_xQL9QnGw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    ## Remove puncuation\n",
        "    text = str(text).translate(string.punctuation)\n",
        "\n",
        "\n",
        "    ## Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\_\", \" _ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"'\\S*@\\S*\\s?'\", '', text)\n",
        "    text = re.sub(r\"'\\s+\", ' ', text)\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    text=re.sub(\"-\",\"\",text)\n",
        "    text=re.sub(\"<\",\"\",text)\n",
        "    text=re.sub(\">\",\"\",text)\n",
        "    text=re.sub(\"</\",\"\",text)\n",
        "    text=re.sub(\"/>\",\"\",text)\n",
        "    text=re.sub(\"_\",\"\",text)\n",
        "    text=re.sub(r\"\\b\\d+\\b\",\"\",text)\n",
        "    text=re.sub(r\"\\b\\_+\\b\",\"\",text)\n",
        "    text = text.lower().split()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    text = [w for w in text if not w in stopwords_list and len(w) > 3]\n",
        "    word_dic = Counter(text)\n",
        "    text = \" \".join(word_dic.keys())\n",
        "    stemmer = PorterStemmer()\n",
        "    words = word_tokenize(text)\n",
        "    text = ' '.join([PorterStemmer().stem(word) for word in words])\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "af-OlRf0Q4a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TOSE8zQXRTen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged['newRisk Sentence']=merged['Risk Sentence'].map(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "R0jlfTwLRUYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def MorePreprocessing(text):\n",
        "    text_new = [t for t in text.split() if t not in stop_words]\n",
        "    text_new = [t for t in text_new if not str.isnumeric(t)]\n",
        "\n",
        "    return text_new\n",
        "\n",
        "merged['Risk Sentence new'] = merged['newRisk Sentence'].apply(MorePreprocessing)"
      ],
      "metadata": {
        "id": "i51Au57wSLs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDpGp3_KIzDs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KYm-RcAhTOiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b693278-b422-485f-f5ea-a610b680c6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "25532    2\n",
            "25533    2\n",
            "25534    2\n",
            "25535    2\n",
            "25536    2\n",
            "Name: Label, Length: 25537, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Split into test/train\n",
        "\n",
        "#Split into test/train\n",
        "X = merged['Risk Sentence']\n",
        "y = merged['Label']\n",
        "\n",
        "\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FpqwE0KMhsxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa1b8aa-431b-49d8-d64f-d0d5309bdd59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['strive eliminate human error training supervision technology redundant process control', 'information regulatory legislative activity area see privacy data protection data governance information cyber security supervision regulation', 'client demand return cash asset particularly limited notice investment pool liquidity support demand could forced sell investment security held asset pool unfavorable price damaging reputation service provider potentially exposing u claim related management pool', 'continually seek improve assumption model may make modification unintentionally cause le predictive may incorrectly interpret data produced model setting credit policy', 'excess spread le zero certain breach representation warranty covenant agreement relating securitization']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "y_train = np.asarray(y_train)\n",
        "y_train\n",
        "\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "X_train = X_train.to_list()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "\n",
        "print(X_train[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lw6n1dfusKrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe351e6e-e5a5-47fc-aeae-3a4eea653b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20429\n"
          ]
        }
      ],
      "source": [
        "#Bert\n",
        "\n",
        "len(X_train)\n",
        "print(len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(y_train)\n",
        "y_train\n",
        "\n",
        "y_test = np.asarray(y_test)"
      ],
      "metadata": {
        "id": "WLVrkGTFIF1u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bert**"
      ],
      "metadata": {
        "id": "WbSK7WZbUzaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = 'bert-base-uncased'\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "bert_model = TFBertModel.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "hWljMqhOKMEa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "model_checkpoints_fin = 'yiyanghkust/finbert-tone'\n",
        "\n",
        "finbert_tokenizer = BertTokenizer.from_pretrained(model_checkpoints_fin)\n",
        "finbert = BertForSequenceClassification.from_pretrained(model_checkpoints_fin)"
      ],
      "metadata": {
        "id": "ySXSvVX2P0Un"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Max\n",
        "\n",
        "max_length = 100\n",
        "\n",
        "train_encodings = bert_tokenizer(X_train, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(X_test, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n"
      ],
      "metadata": {
        "id": "TpNzT2a0LBdR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UBwSXUzLylz",
        "outputId": "05594e54-01da-4d78-a069-20fea5af04ba"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
              " array([[  101, 29453, 11027,  2529,  7561,  2731, 10429,  2974, 21707,\n",
              "          2832,  2491,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0]], dtype=int32)>,\n",
              " 'token_type_ids': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
              " 'attention_mask': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bert_multiclass_model(checkpoint = model_checkpoint,\n",
        "                                 num_classes = 4,\n",
        "                                 hidden_size = 201,\n",
        "                                 dropout=0.3,\n",
        "                                 learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes.\n",
        "    \"\"\"\n",
        "    bert_model = TFBertModel.from_pretrained(checkpoint)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name ='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    pooler_token = bert_out[1]\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes , activation='softmax',name='classification_layer')(hidden)\n",
        "    #predicted_class = tf.argmax(classification, axis=1)  # Get the index of the class with highest probability\n",
        "    #predicted_class = tf.cast(predicted_class, dtype=tf.int64)\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs = classification)\n",
        "\n",
        "\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics='accuracy')\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "    return classification_model"
      ],
      "metadata": {
        "id": "lfQ7WoPzLbBW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooler_bert_model = create_bert_multiclass_model(checkpoint=model_checkpoint, num_classes=4, hidden_size = 768)"
      ],
      "metadata": {
        "id": "ZiiUGbKeLeGH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooler_bert_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuj_j5iVMJ3X",
        "outputId": "22969dd4-2263-42fc-9b89-cd3b75b6358e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " attention_mask_layer (Inpu  [(None, 100)]                0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " input_ids_layer (InputLaye  [(None, 100)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " token_type_ids_layer (Inpu  [(None, 100)]                0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_bert_model_4 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['attention_mask_layer[0][0]',\n",
            " el)                         ngAndCrossAttentions(last_   40         'input_ids_layer[0][0]',     \n",
            "                             hidden_state=(None, 100, 7              'token_type_ids_layer[0][0]']\n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)        (None, 768)                  590592    ['tf_bert_model_4[0][1]']     \n",
            "                                                                                                  \n",
            " dropout_187 (Dropout)       (None, 768)                  0         ['hidden_layer[0][0]']        \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 4)                    3076      ['dropout_187[0][0]']         \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 110075908 (419.91 MB)\n",
            "Trainable params: 110075908 (419.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooler_bert_model_history = pooler_bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask],\n",
        "                                                  y_train,\n",
        "                                                  validation_data=([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask],\n",
        "                                                  y_test),\n",
        "                                                  batch_size=8,\n",
        "                                                  epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu20V36XMhr2",
        "outputId": "4967cfde-3f59-4aeb-8bda-4e04c2cf0f33"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2554/2554 [==============================] - 232s 74ms/step - loss: 0.9506 - accuracy: 0.5785 - val_loss: 0.6546 - val_accuracy: 0.7574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooler_bert_model_history = pooler_bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask],\n",
        "                                                  y_train,\n",
        "                                                  validation_data=([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask],\n",
        "                                                  y_test),\n",
        "                                                  batch_size=8,\n",
        "                                                  epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hy8r-a5N3jq",
        "outputId": "404ed4bb-cfd3-4704-c187-dc95c84ebb7d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2554/2554 [==============================] - 164s 64ms/step - loss: 0.4865 - accuracy: 0.8258 - val_loss: 0.3779 - val_accuracy: 0.8726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pooler_bert_model.predict([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask])\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "classification_rep = classification_report(y_test, predicted_labels)\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orgwZowtgRL3",
        "outputId": "b7f356e0-e448-4ed6-c1fc-93f782321db4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 8s 34ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86      1279\n",
            "           1       0.91      0.82      0.86      1046\n",
            "           2       0.86      0.86      0.86      1412\n",
            "           3       0.90      0.92      0.91      1371\n",
            "\n",
            "    accuracy                           0.87      5108\n",
            "   macro avg       0.88      0.87      0.87      5108\n",
            "weighted avg       0.87      0.87      0.87      5108\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINBERT"
      ],
      "metadata": {
        "id": "sY87e5Z_QeCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_train_encodings = finbert_tokenizer(X_train, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "finbert_test_encodings = finbert_tokenizer(X_test, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "pR-FRN1MQe1d"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_train_encodings[:1]"
      ],
      "metadata": {
        "id": "D6x34h5vRKBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410aab4b-0006-4270-ec3c-1dad649da669"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
              " array([[    3,  9246,  3613,  1891,  9221,  2274,  5232,   341, 13578,\n",
              "           529,   527,     4,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0]], dtype=int32)>,\n",
              " 'token_type_ids': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
              " 'attention_mask': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JETsJVeg0l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_finbert_multiclass_model(checkpoint = model_checkpoints_fin,\n",
        "                                    num_classes=4,\n",
        "                                    hidden_size=1024,\n",
        "                                    dropout=0.3,\n",
        "                                    learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with FinBERT. Use the Pooler Output for classification purposes.\n",
        "    \"\"\"\n",
        "    # Load the FinBERT tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "    finbert_model = TFBertModel.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "    # Define the inputs\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    # Build the model\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}\n",
        "    bert_out = finbert_model(bert_inputs)\n",
        "    pooler_token = bert_out[1]\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(hidden)\n",
        "\n",
        "    # Create the model\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=classification)\n",
        "\n",
        "    # Compile the model\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model\n"
      ],
      "metadata": {
        "id": "QF8US9YTQlLk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7pc0QtcsR4N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_model = create_finbert_multiclass_model(model_checkpoints_fin, num_classes=4, hidden_size = 768)"
      ],
      "metadata": {
        "id": "8PWgyZDrR43f"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_model_history = finbert_model.fit([finbert_train_encodings.input_ids, finbert_train_encodings.token_type_ids, finbert_train_encodings.attention_mask],\n",
        "                                                  y_train,\n",
        "                                                  validation_data=([finbert_test_encodings.input_ids, finbert_test_encodings.token_type_ids, finbert_test_encodings.attention_mask],\n",
        "                                                  y_test),\n",
        "                                                  batch_size=8,\n",
        "                                                  epochs=1)"
      ],
      "metadata": {
        "id": "D9WdeaWoSJdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b3872f-4be8-4653-b418-7891b40ae045"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2554/2554 [==============================] - 232s 74ms/step - loss: 0.7663 - accuracy: 0.6846 - val_loss: 0.5330 - val_accuracy: 0.8107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the trained finbert_model\n",
        "finbert_predictions = finbert_model.predict([finbert_test_encodings.input_ids, finbert_test_encodings.token_type_ids, finbert_test_encodings.attention_mask])\n",
        "finbert_predicted_labels = np.argmax(finbert_predictions, axis=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "finbert_classification_rep = classification_report(y_test, finbert_predicted_labels)\n",
        "print(finbert_classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD3_O6RHg2Dm",
        "outputId": "94cb0788-453a-41fd-f5e8-4ec636d9631d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 8s 33ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.80      0.83      1279\n",
            "           1       0.85      0.79      0.82      1046\n",
            "           2       0.89      0.70      0.79      1412\n",
            "           3       0.71      0.94      0.81      1371\n",
            "\n",
            "    accuracy                           0.81      5108\n",
            "   macro avg       0.83      0.81      0.81      5108\n",
            "weighted avg       0.82      0.81      0.81      5108\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN **"
      ],
      "metadata": {
        "id": "gcIk3sKBfP7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Flatten\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)  # Assuming a vocabulary size of 10,000\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=100, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=100, padding='post')\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "y_train_encoded = to_categorical(y_train)\n",
        "y_test_encoded = to_categorical(y_test)\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=100, input_length=100))\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))  # 4 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded),\n",
        "          epochs=10, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "M--A1deQW9Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c0bdba-fc93-4116-a207-7a03e7608fe2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 98, 128)           38528     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1047044 (3.99 MB)\n",
            "Trainable params: 1047044 (3.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "639/639 [==============================] - 24s 35ms/step - loss: 0.7032 - accuracy: 0.7195 - val_loss: 0.3620 - val_accuracy: 0.8823\n",
            "Epoch 2/10\n",
            "639/639 [==============================] - 7s 11ms/step - loss: 0.1674 - accuracy: 0.9507 - val_loss: 0.2187 - val_accuracy: 0.9370\n",
            "Epoch 3/10\n",
            "639/639 [==============================] - 4s 6ms/step - loss: 0.0521 - accuracy: 0.9875 - val_loss: 0.1980 - val_accuracy: 0.9503\n",
            "Epoch 4/10\n",
            "639/639 [==============================] - 4s 7ms/step - loss: 0.0290 - accuracy: 0.9946 - val_loss: 0.1980 - val_accuracy: 0.9548\n",
            "Epoch 5/10\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.0173 - accuracy: 0.9971 - val_loss: 0.2063 - val_accuracy: 0.9530\n",
            "Epoch 6/10\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.2207 - val_accuracy: 0.9522\n",
            "Epoch 7/10\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.0136 - accuracy: 0.9972 - val_loss: 0.2155 - val_accuracy: 0.9538\n",
            "Epoch 8/10\n",
            "639/639 [==============================] - 4s 6ms/step - loss: 0.0151 - accuracy: 0.9963 - val_loss: 0.2496 - val_accuracy: 0.9475\n",
            "Epoch 9/10\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.0270 - accuracy: 0.9924 - val_loss: 0.2770 - val_accuracy: 0.9385\n",
            "Epoch 10/10\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.0181 - accuracy: 0.9943 - val_loss: 0.2637 - val_accuracy: 0.9503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d3afa2c0220>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_test_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded test labels back to original labels\n",
        "y_test_original = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test_original, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test_original, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test_original, y_pred_classes, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6uqupJLhlPy",
        "outputId": "2803d983-ec8f-401a-b5a4-61bb3aa8e107"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 0s 1ms/step\n",
            "Precision: 0.9502848594980123\n",
            "Recall: 0.9502740798747064\n",
            "F1-score: 0.9502192330005236\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}